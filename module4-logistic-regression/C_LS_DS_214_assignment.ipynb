{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_214_assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jduell12/DS-Unit-2-Linear-Models/blob/master/C_LS_DS_214_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP4xoliz9InQ"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 2, Sprint 1, Module 4*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6DvcJCM9InT"
      },
      "source": [
        "%%capture\n",
        "import sys\n",
        "\n",
        "# If you're on Colab:\n",
        "if 'google.colab' in sys.modules:\n",
        "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Linear-Models/master/data/'\n",
        "    !pip install category_encoders==2.*\n",
        "\n",
        "# If you're working locally:\n",
        "else:\n",
        "    DATA_PATH = '../data/'"
      ],
      "execution_count": 362,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw4_7zqc9InU"
      },
      "source": [
        "# Module Project: Logistic Regression\n",
        "\n",
        "Do you like burritos? ðŸŒ¯ You're in luck then, because in this project you'll create a model to predict whether a burrito is `'Great'`.\n",
        "\n",
        "The dataset for this assignment comes from [Scott Cole](https://srcole.github.io/100burritos/), a San Diego-based data scientist and burrito enthusiast. \n",
        "\n",
        "## Directions\n",
        "\n",
        "The tasks for this project are the following:\n",
        "\n",
        "- **Task 1:** Import `csv` file using `wrangle` function.\n",
        "- **Task 2:** Conduct exploratory data analysis (EDA), and modify `wrangle` function .\n",
        "- **Task 3:** Split data into feature matrix `X` and target vector `y`.\n",
        "- **Task 4:** Split feature matrix `X` and target vector `y` into training and test sets.\n",
        "- **Task 5:** Establish the baseline accuracy score for your dataset.\n",
        "- **Task 6:** Build `model_logr` using a pipeline that includes three transfomers and `LogisticRegression` predictor. Train model on `X_train` and `X_test`.\n",
        "- **Task 7:** Calculate the training and test accuracy score for your model.\n",
        "- **Task 8:** Create a horizontal bar chart showing the 10 most influencial features for your  model. \n",
        "- **Task 9:** Demonstrate and explain the differences between `model_lr.predict()` and `model_lr.predict_proba()`.\n",
        "\n",
        "**Note** \n",
        "\n",
        "You should limit yourself to the following libraries:\n",
        "\n",
        "- `category_encoders`\n",
        "- `matplotlib`\n",
        "- `pandas`\n",
        "- `sklearn`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDr5DxnqOt8o"
      },
      "source": [
        "from category_encoders import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "execution_count": 363,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g52j8Qub9InV"
      },
      "source": [
        "# I. Wrangle Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wijcmL8U9InW"
      },
      "source": [
        "def isChecked(cell):\n",
        "  cell = cell.lower()\n",
        "  if cell == 'x' or cell == 'yes':\n",
        "    return 1\n",
        "  return 0\n",
        "\n",
        "def isCal(cell):\n",
        "  if cell == 'California':\n",
        "    return 1\n",
        "  return 0\n",
        "\n",
        "def isAsada(cell):\n",
        "  if cell == 'Asada':\n",
        "    return 1\n",
        "  return 0\n",
        "\n",
        "def isCarnitas(cell):\n",
        "  if cell == 'Carnitas':\n",
        "    return 1\n",
        "  return 0\n",
        "\n",
        "def isSurf(cell):\n",
        "  if cell == 'Surf & Turf':\n",
        "    return 1 \n",
        "  return 0\n",
        "\n",
        "def wrangle(filepath):\n",
        "    # Import w/ DateTimeIndex\n",
        "    df = pd.read_csv(filepath, parse_dates=['Date'],\n",
        "                     index_col='Date').sort_index()\n",
        "    \n",
        "    # Drop unrated burritos\n",
        "    df.dropna(subset=['overall'], inplace=True)\n",
        "    \n",
        "    # Derive binary classification target:\n",
        "    # We define a 'Great' burrito as having an\n",
        "    # overall rating of 4 or higher, on a 5 point scale\n",
        "    df['Great'] = (df['overall'] >= 4).astype(int)\n",
        "    \n",
        "    # Drop high cardinality categoricals\n",
        "    df = df.drop(columns=['Notes', 'Location', 'Address', 'URL', 'Neighborhood', 'Reviewer'])\n",
        "    \n",
        "    # Drop columns to prevent \"leakage\"\n",
        "    df = df.drop(columns=['Rec', 'overall'])\n",
        "\n",
        "    #replace NaN with 0\n",
        "    df = df.fillna('0')\n",
        "\n",
        "    # change columns with object appropiate types\n",
        "    cols = ['Fries','Cheese','Guac','Pico','NonSD','Unreliable','Beef','Chips','Sour cream', 'Pork', 'Chicken', 'Shrimp', 'Fish', 'Rice', 'Beans',\n",
        "       'Lettuce', 'Tomato', 'Bell peper', 'Carrots', 'Cabbage', 'Sauce',\n",
        "       'Salsa.1', 'Cilantro', 'Onion', 'Taquito', 'Pineapple', 'Ham',\n",
        "       'Chile relleno', 'Nopales', 'Lobster', 'Queso', 'Egg', 'Mushroom',\n",
        "       'Bacon', 'Sushi', 'Avocado', 'Corn', 'Zucchini']\n",
        "    for col in cols:\n",
        "      df[col] = df[col].apply(isChecked)\n",
        "\n",
        "    float_cols = ['Yelp', 'Google', 'Cost', 'Hunger', 'Mass (g)', 'Density (g/mL)', 'Length', 'Circum', 'Volume', 'Temp', 'Meat', 'Fillings', 'Meat:filling', 'Uniformity', 'Salsa', 'Synergy', 'Wrap']\n",
        "    for col in float_cols:\n",
        "      df[col] = df[col].apply(float)\n",
        "\n",
        "    # Change the wrangle function above so that it engineers four new features: 'california', 'asada', 'surf', and 'carnitas'. \n",
        "    # Each row should have a 1 or 0 based on the text information in the 'Burrito' column. \n",
        "    df['Burrito'] = df['Burrito'].str.lower()\n",
        "\n",
        "    california = df['Burrito'].str.contains('california')\n",
        "    asada = df['Burrito'].str.contains('asada')\n",
        "    surf = df['Burrito'].str.contains('surf')\n",
        "    carnitas = df['Burrito'].str.contains('carnitas')\n",
        "\n",
        "    df.loc[california, 'Burrito'] = 'California'\n",
        "    df.loc[asada, 'Burrito'] = 'Asada'\n",
        "    df.loc[surf, 'Burrito'] = 'Surf & Turf'\n",
        "    df.loc[carnitas, 'Burrito'] = 'Carnitas'\n",
        "    df.loc[~california & ~asada & ~surf & ~carnitas, 'Burrito'] = 'Other'\n",
        "\n",
        "    df['california'] = df['Burrito'].apply(isCal)\n",
        "    df['asada'] = df['Burrito'].apply(isAsada)\n",
        "    df['surf'] = df['Burrito'].apply(isSurf)\n",
        "    df['carnitas'] = df['Burrito'].apply(isCarnitas)\n",
        "\n",
        "    df = df.drop(columns=['Burrito'])\n",
        "    \n",
        "    return df\n",
        "\n",
        "filepath = DATA_PATH + 'burritos/burritos.csv'"
      ],
      "execution_count": 364,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFyGMDrO9InW"
      },
      "source": [
        "**Task 1:** Use the above `wrangle` function to import the `burritos.csv` file into a DataFrame named `df`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywx46UiN9InX"
      },
      "source": [
        "filepath = DATA_PATH + 'burritos/burritos.csv'\n",
        "df = wrangle(filepath)"
      ],
      "execution_count": 365,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKCV_XGwYujh"
      },
      "source": [
        "# df.tail()"
      ],
      "execution_count": 366,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti97i2b29InX"
      },
      "source": [
        "During your exploratory data analysis, note that there are several columns whose data type is `object` but that seem to be a binary encoding. For example, `df['Beef'].head()` returns:\n",
        "\n",
        "```\n",
        "0      x\n",
        "1      x\n",
        "2    NaN\n",
        "3      x\n",
        "4      x\n",
        "Name: Beef, dtype: object\n",
        "```\n",
        "\n",
        "**Task 2:** Change the `wrangle` function so that these columns are properly encoded as `0` and `1`s. Be sure your code handles upper- and lowercase `X`s, and `NaN`s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOocAarO9InY",
        "outputId": "99a60bac-3f59-4cea-dff0-7a5fd5d3658d"
      },
      "source": [
        "# Conduct your exploratory data analysis here\n",
        "# And modify the `wrangle` function above.\n",
        "\n",
        "df.select_dtypes(include=['object']).columns\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 367,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index([], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 367
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anlxOH2N9InY"
      },
      "source": [
        "If you explore the `'Burrito'` column of `df`, you'll notice that it's a high-cardinality categorical feature. You'll also notice that there's a lot of overlap between the categories. \n",
        "\n",
        "**Stretch Goal:** Change the `wrangle` function above so that it engineers four new features: `'california'`, `'asada'`, `'surf'`, and `'carnitas'`. Each row should have a `1` or `0` based on the text information in the `'Burrito'` column. For example, here's how the first 5 rows of the dataset would look.\n",
        "\n",
        "| **Burrito** | **california** | **asada** | **surf** | **carnitas** |\n",
        "| :---------- | :------------: | :-------: | :------: | :----------: |\n",
        "| California  |       1        |     0     |    0     |      0       |\n",
        "| California  |       1        |     0     |    0     |      0       |\n",
        "|  Carnitas   |       0        |     0     |    0     |      1       |\n",
        "| Carne asada |       0        |     1     |    0     |      0       |\n",
        "| California  |       1        |     0     |    0     |      0       |\n",
        "\n",
        "**Note:** Be sure to also drop the `'Burrito'` once you've engineered your new features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI5jTTA_9InZ"
      },
      "source": [
        "# II. Split Data\n",
        "\n",
        "**Task 3:** Split your dataset into the feature matrix `X` and the target vector `y`. You want to predict `'Great'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDVTFaSK9Ina"
      },
      "source": [
        "target = 'Great'\n",
        "y = df[target]\n",
        "X = df.drop(columns=target)"
      ],
      "execution_count": 368,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "jsEO-mQsY3Ry",
        "outputId": "0de3cdc5-00bc-4e72-bf39-9f8b21a492f9"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": 369,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Yelp</th>\n",
              "      <th>Google</th>\n",
              "      <th>Chips</th>\n",
              "      <th>Cost</th>\n",
              "      <th>Hunger</th>\n",
              "      <th>Mass (g)</th>\n",
              "      <th>Density (g/mL)</th>\n",
              "      <th>Length</th>\n",
              "      <th>Circum</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Tortilla</th>\n",
              "      <th>Temp</th>\n",
              "      <th>Meat</th>\n",
              "      <th>Fillings</th>\n",
              "      <th>Meat:filling</th>\n",
              "      <th>Uniformity</th>\n",
              "      <th>Salsa</th>\n",
              "      <th>Synergy</th>\n",
              "      <th>Wrap</th>\n",
              "      <th>Unreliable</th>\n",
              "      <th>NonSD</th>\n",
              "      <th>Beef</th>\n",
              "      <th>Pico</th>\n",
              "      <th>Guac</th>\n",
              "      <th>Cheese</th>\n",
              "      <th>Fries</th>\n",
              "      <th>Sour cream</th>\n",
              "      <th>Pork</th>\n",
              "      <th>Chicken</th>\n",
              "      <th>Shrimp</th>\n",
              "      <th>Fish</th>\n",
              "      <th>Rice</th>\n",
              "      <th>Beans</th>\n",
              "      <th>Lettuce</th>\n",
              "      <th>Tomato</th>\n",
              "      <th>Bell peper</th>\n",
              "      <th>Carrots</th>\n",
              "      <th>Cabbage</th>\n",
              "      <th>Sauce</th>\n",
              "      <th>Salsa.1</th>\n",
              "      <th>Cilantro</th>\n",
              "      <th>Onion</th>\n",
              "      <th>Taquito</th>\n",
              "      <th>Pineapple</th>\n",
              "      <th>Ham</th>\n",
              "      <th>Chile relleno</th>\n",
              "      <th>Nopales</th>\n",
              "      <th>Lobster</th>\n",
              "      <th>Queso</th>\n",
              "      <th>Egg</th>\n",
              "      <th>Mushroom</th>\n",
              "      <th>Bacon</th>\n",
              "      <th>Sushi</th>\n",
              "      <th>Avocado</th>\n",
              "      <th>Corn</th>\n",
              "      <th>Zucchini</th>\n",
              "      <th>california</th>\n",
              "      <th>asada</th>\n",
              "      <th>surf</th>\n",
              "      <th>carnitas</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2011-05-16</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.00</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-04-20</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-18</th>\n",
              "      <td>3.5</td>\n",
              "      <td>4.2</td>\n",
              "      <td>0</td>\n",
              "      <td>6.49</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-24</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>5.25</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-24</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4.85</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Yelp  Google  Chips  Cost  ...  california  asada  surf  carnitas\n",
              "Date                                   ...                                   \n",
              "2011-05-16   0.0     0.0      0  8.00  ...           0      0     0         0\n",
              "2015-04-20   0.0     0.0      0  0.00  ...           0      0     0         0\n",
              "2016-01-18   3.5     4.2      0  6.49  ...           1      0     0         0\n",
              "2016-01-24   0.0     0.0      0  5.25  ...           0      1     0         0\n",
              "2016-01-24   0.0     0.0      0  4.85  ...           0      0     0         1\n",
              "\n",
              "[5 rows x 60 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 369
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hadh_GDJZlfP",
        "outputId": "f7d4b5db-bb70-48d7-950b-a867f4b0912a"
      },
      "source": [
        "y.head()"
      ],
      "execution_count": 370,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date\n",
              "2011-05-16    0\n",
              "2015-04-20    1\n",
              "2016-01-18    0\n",
              "2016-01-24    0\n",
              "2016-01-24    0\n",
              "Name: Great, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 370
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBNt61zN9Ina"
      },
      "source": [
        "**Task 4:** Split `X` and `y` into a training set (`X_train`, `y_train`) and a test set (`X_test`, `y_test`).\n",
        "\n",
        "- Your training set should include data from 2016 through 2017. \n",
        "- Your test set should include data from 2018 and later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLBMLFl29Ina"
      },
      "source": [
        "cutoff = '2017-12-31'\n",
        "mask = X.index < cutoff\n",
        "X_train, y_train = X.loc[mask], y.loc[mask]\n",
        "X_test, y_test = X.loc[~mask], y.loc[~mask]"
      ],
      "execution_count": 371,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvDRMag-Yg4R",
        "outputId": "01d1b563-0eba-4819-fed7-6692f39802e9"
      },
      "source": [
        "y_train.tail()"
      ],
      "execution_count": 372,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date\n",
              "2017-09-05    1\n",
              "2017-09-05    1\n",
              "2017-12-16    0\n",
              "2017-12-29    1\n",
              "2017-12-29    1\n",
              "Name: Great, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 372
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxgVakEpYjvm",
        "outputId": "90b44831-1a10-483b-ddf6-ea256b6c9d3b"
      },
      "source": [
        "y_test.head()"
      ],
      "execution_count": 373,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date\n",
              "2018-01-02    0\n",
              "2018-01-09    0\n",
              "2018-01-12    0\n",
              "2018-01-12    1\n",
              "2018-04-04    1\n",
              "Name: Great, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 373
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5s0YUlN9Inb"
      },
      "source": [
        "# III. Establish Baseline\n",
        "\n",
        "**Task 5:** Since this is a **classification** problem, you should establish a baseline accuracy score. Figure out what is the majority class in `y_train` and what percentage of your training observations it represents. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28Xb_JbJ9Inb",
        "outputId": "c6a2b5ae-217d-47e6-c3c7-17658116a053"
      },
      "source": [
        "baseline_acc = y_train.value_counts(normalize=True).max()\n",
        "print('Baseline Accuracy Score:', baseline_acc)\n",
        "\n"
      ],
      "execution_count": 374,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Accuracy Score: 0.5822454308093995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o99gi0Kc9Inb"
      },
      "source": [
        "# IV. Build Model\n",
        "\n",
        "**Task 6:** Build a `Pipeline` named `model_logr`, and fit it to your training data. Your pipeline should include:\n",
        "\n",
        "- a `OneHotEncoder` transformer for categorical features, \n",
        "- a `SimpleImputer` transformer to deal with missing values, \n",
        "- a [`StandarScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) transfomer (which often improves performance in a logistic regression model), and \n",
        "- a `LogisticRegression` predictor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xF-mSuai9Inb",
        "outputId": "39d7bcf3-5533-4f92-8310-34ef862aabd6"
      },
      "source": [
        "model_logr = make_pipeline(\n",
        "    OneHotEncoder(use_cat_names=True),\n",
        "    SimpleImputer(),\n",
        "    StandardScaler(),\n",
        "    LogisticRegression()\n",
        ")\n",
        "\n",
        "model_logr.fit(X_train, y_train)"
      ],
      "execution_count": 375,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
            "  elif pd.api.types.is_categorical(cols):\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('onehotencoder',\n",
              "                 OneHotEncoder(cols=[], drop_invariant=False,\n",
              "                               handle_missing='value', handle_unknown='value',\n",
              "                               return_df=True, use_cat_names=True, verbose=0)),\n",
              "                ('simpleimputer',\n",
              "                 SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
              "                               missing_values=nan, strategy='mean',\n",
              "                               verbose=0)),\n",
              "                ('standardscaler',\n",
              "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                ('logisticregression',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='auto', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 375
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAlbzfd69Inc"
      },
      "source": [
        "# IV. Check Metrics\n",
        "\n",
        "**Task 7:** Calculate the training and test accuracy score for `model_lr`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arjl-Ncq9Inc",
        "outputId": "06009b56-eb48-49da-eaac-38390f8bc187"
      },
      "source": [
        "training_acc = model_logr.score(X_train, y_train)\n",
        "test_acc = model_logr.score(X_test, y_test)\n",
        "\n",
        "print('Training MAE:', training_acc)\n",
        "print('Test MAE:', test_acc)"
      ],
      "execution_count": 376,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training MAE: 0.8955613577023499\n",
            "Test MAE: 0.7368421052631579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP61D1gn9Inc"
      },
      "source": [
        "# V. Communicate Results\n",
        "\n",
        "**Task 8:** Create a horizontal barchart that plots the 10 most important coefficients for `model_lr`, sorted by absolute value.\n",
        "\n",
        "**Note:** Since you created your model using a `Pipeline`, you'll need to use the [`named_steps`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) attribute to access the coefficients in your `LogisticRegression` predictor. Be sure to look at the shape of the coefficients array before you combine it with the feature names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AyDOH2saPXP",
        "outputId": "b8cdd46d-aad2-4dc4-e46d-d922d308ced8"
      },
      "source": [
        "model_logr.named_steps"
      ],
      "execution_count": 377,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'logisticregression': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                    warm_start=False),\n",
              " 'onehotencoder': OneHotEncoder(cols=[], drop_invariant=False, handle_missing='value',\n",
              "               handle_unknown='value', return_df=True, use_cat_names=True,\n",
              "               verbose=0),\n",
              " 'simpleimputer': SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
              "               missing_values=nan, strategy='mean', verbose=0),\n",
              " 'standardscaler': StandardScaler(copy=True, with_mean=True, with_std=True)}"
            ]
          },
          "metadata": {},
          "execution_count": 377
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7n3n2Q6aTxu",
        "outputId": "b827620e-ac49-47cd-a24e-b7d6f4ea0569"
      },
      "source": [
        "coeff = model_logr['logisticregression'].coef_[0]\n",
        "features = model_logr['onehotencoder'].get_feature_names()\n",
        "\n",
        "print(len(coeff))\n",
        "print(len(features))\n"
      ],
      "execution_count": 378,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60\n",
            "60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RucPZbHHaiAR"
      },
      "source": [
        "feature_imp = pd.Series(coeff, index=features).sort_values(key=abs)"
      ],
      "execution_count": 379,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Ouqb-Q_79Inc",
        "outputId": "6c5e167f-4650-40fc-934b-befcd211238f"
      },
      "source": [
        "# Create your horizontal barchart here.\n",
        "feature_imp.tail(10).plot(kind='barh')"
      ],
      "execution_count": 380,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe727d61710>"
            ]
          },
          "metadata": {},
          "execution_count": 380
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24XESqVs9Inc"
      },
      "source": [
        "There is more than one way to generate predictions with `model_lr`. For instance, you can use [`predict`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression) or [`predict_proba`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression#sklearn.linear_model.LogisticRegression.predict_proba).\n",
        "\n",
        "**Task 9:** Generate predictions for `X_test` using both `predict` and `predict_proba`. Then below, write a summary of the differences in the output for these two methods. You should answer the following questions:\n",
        "\n",
        "- What data type do `predict` and `predict_proba` output?\n",
        "- What are the shapes of their different output?\n",
        "- What numerical values are in the output?\n",
        "- What do those numerical values represent?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTxSsIRt9Ind",
        "outputId": "0e6c92c2-e5ac-496f-ab06-6c086e5adf42"
      },
      "source": [
        "# Write code here to explore the differences between `predict` and `predict_proba`.\n",
        "predict = model_logr.predict(X_test)\n",
        "type(predict[0])"
      ],
      "execution_count": 387,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.int64"
            ]
          },
          "metadata": {},
          "execution_count": 387
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBKYI9yV2iJN",
        "outputId": "c77042a5-94d4-43ef-aee3-42044bd64681"
      },
      "source": [
        "predict_proba = model_logr.predict_proba(X_test)\n",
        "np.sort(predict_proba).max()\n"
      ],
      "execution_count": 403,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999964289879676"
            ]
          },
          "metadata": {},
          "execution_count": 403
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMpHPhmI9Ind"
      },
      "source": [
        "**Give your written answer here:**\n",
        "* What data type do predict and predict_proba output?\n",
        "\n",
        "Predict is an int while predict_proba is a float\n",
        "\n",
        "* What are the shapes of their different output?\n",
        "\n",
        "Both have a length of 38 in their array.\n",
        "\n",
        "* What numerical values are in the output?\n",
        "\n",
        "Predict has either 1 or 0 and predict_proba has a range from almost 0 to almost 1.\n",
        "\n",
        "* What do those numerical values represent?\n",
        "Predict returns either a 1 or 0 which represents if that burrito will be Great or not. Predict_porba returns a float which represents the probability estimates for each column. "
      ]
    }
  ]
}